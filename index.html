<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ziyi Yang's homepage</title>

  <meta name="author" content="Ziyi Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ziyi Yang (Êù®Â≠êÈÄ∏)</name>
              </p>
              <p>My name is Ziyi Yang. I am a second-year MS student (expected to graduate in 2026) at Sun Yat-sen University, advised by <a href="https://sites.google.com/site/xiaojunquan/">Prof. Xiaojun Quan</a>. Before this, I received my Bachelor's degree (2019-2023, computer science and technology) from Sun Yat-sen University. I am currently an intern at Alibaba Tongyi (2025.05-now).
                </p>
              <p style="text-align:center">
                  <a href="yanzy39@mail2.sysu.edu.cn">Email</a>  /
                  <a href="images/mycv.pdf">CV</a>  /
                  <a href="https://scholar.google.com/citations?user=DKNpsvgAAAAJ&hl=zh-CN">Google Scholar</a>  /
                  <a href="https://github.com/yangzy39">GitHub</a>  /
		  <a href="https://huggingface.co/AALF">HF</a>  
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/yangzy.jpg"><img style="width:50%;max-width:50%" alt="profile photo" src="images/yangzy.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
               My primary research interests lie at the intersection of several key areas in modern artificial intelligence. These include heterogeneous model fusion, with a focus on integrating diverse large language models (LLMs); advanced preference learning algorithms such as DPO and SimPO; the development of large reasoning models (LRMs) capable of adaptive thinking; and novel reinforcement learning (RL) methodologies, particularly in long-context and self-play scenarios. My representative publications are listed below.
              </p>
            </td>
          </tr>
        </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
    <tr>
      <td colspan="2" style="padding:20px;text-align:left;font-weight:bold;background-color:#ffffff;">
        Knowledge Fusion & Preference Learning
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/wrpo.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2412.03187" id="conf_wrpo">
          <papertitle><span class="highlight">Weighted-Reward Preference Optimization for Implicit Model Fusion</span></papertitle>
        </a>
        <br>
	<strong>Ziyi Yang</strong>,
        Fanqi Wan,
	Longguang Zhong,
	Tianyuan Shi,
	Xiaojun Quan
        <br>
        <strong>ICLR</strong>, 2025
        <br>
        <a href="https://arxiv.org/abs/2412.03187">[Paper]</a>
	/
        <a href="https://github.com/yangzy39/WRPO">[GitHub]</a>
        /
	<a href="https://huggingface.co/datasets/AALF/ultrafeedback_wrpo">[HF]</a>
        <p></p>
        <p>We propose an implicit fusion method, Weighted-Reward Preference Optimization (WRPO), which leverages preference optimization between the source LLMs and the target LLM to transfer their capabilities effectively. WRPO achieves <strong>a LC Win Rate of 55.9% against GPT-4-Preview-1106 on AlpacaEval-2 and a Win Rate of 46.2% against GPT-4-0314 on Arena-Hard</strong>.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/FuseChat-3.0.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/pdf/2503.04222" id="conf_wrpo">
          <papertitle><span class="highlight">FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion</span></papertitle>
        </a>
        <br>
	<strong>Ziyi Yang</strong>,
        Fanqi Wan,
	Longguang Zhong,
	Canbin Huang,
    Guosheng Liang,
	Xiaojun Quan
        <br>
        <strong>SCI-FM @ ICLR</strong>, 2025
        <br>
	<a href="https://arxiv.org/pdf/2503.04222">[Paper]</a>
        /
	<a href="https://huggingface.co/collections/FuseAI/fusechat-30-6752d18dec430bad7a236a75">[HF]</a>
        /
	<a href="https://huggingface.co/papers/2503.04222">[HF Daily Papers]</a>
	/
	<a href="https://www.reddit.com/r/LocalLLaMA/comments/1hd22cq/fusechat30_preference_optimization_for_implicit/?rdt=45331">[r/LocalLLaMA]</a>
	/
        <a href="https://github.com/yangzy39/FuseChat-3.0">[GitHub]</a>
        /
	<a href="https://mp.weixin.qq.com/s/xI_GLkO0eskwayV_TveuUg">[È≠îÊê≠Á§æÂå∫]</a>
        <p></p>
        <p>We introduce FuseChat-3.0, a suite of large language models (LLMs) developed by integrating the strengths of heterogeneous source LLMs into more compact target LLMs. Using Llama-3.1-8B-Instruct as the target model, our fusion approach achieves <strong>an average improvement of 6.8 points across 14 benchmarks</strong>.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/mutual.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://openreview.net/forum?id=IT2H83KYpc&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Daclweb.org%2FACL%2F2025%2FConference%2FAuthors%23your-submissions)" id="conf_mutual">
          <papertitle>Mutual-Taught for Co-adapting Policy and Reward Models</papertitle>
        </a>
        <br>
	Tianyuan Shi,
	Canbin Huang, 
	Fanqi Wa,
	Longguang Zhong, 
	<strong>Ziyi Yang</strong>,
	Weizhou Shen, 
	Xiaojun Quan,
	Ming Yan
        <br>
        <strong>ACL Main</strong> , 2025
        <br>
        <a href="https://arxiv.org/pdf/2506.06292">[Paper]</a>
        <p></p>
        <p>We propose Mutual-Taught, a self-training method that iteratively improves both the policy model and reward model without requiring additional human annotation. Our approach mirrors the expectation-maximization (EM) algorithm. Experimental results demonstrate that this iterative approach leads to <strong>consistent improvements in both models</strong>.</p>
      </td>
    </tr>
	    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/fuserl.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2504.06562" id="conf_fusechat">
          <papertitle>FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion</papertitle>
        </a>
        <br>
	Longguang Zhong,
        Fanqi Wan,
	<strong>Ziyi Yang</strong>,
	Guosheng Liang,
	Tianyuan Shi,
	Xiaojun Quan
        <br>
        <em>Preprint</em>, 2025
        <br>
        <p></p>
		  <a href="https://arxiv.org/abs/2504.06562">[Paper]</a>
        <p>We propose FuseRL, a novel two-stage framework comprising FuseSFT and FusePO to maximize the utilization of source LLMs. Using Llama-3.1-8B-Instruct as the target model, our approach achieves <strong>state-of-the-art performance among 8B LLMs on AlpacaEval-2 and Arena-Hard</strong>.</p>
      </td>
    </tr>

      <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/fusechat.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2408.07990" id="conf_fusechat">
          <papertitle>FuseChat: Knowledge Fusion of Chat Models</papertitle>
        </a>
        <br>
        Fanqi Wan,
	Longguang Zhong,
	<strong>Ziyi Yang</strong>,
	Ruijun Chen,
	Xiaojun Quan
        <br>
        <em>Tech Report</em>, 2024
        <br>
        <a href="https://arxiv.org/abs/2408.07990">[Paper]</a>
	/
        <a href="https://github.com/fanqiwan/FuseAI/tree/main/FuseChat">[GitHub]</a>
        /
	<a href="https://huggingface.co/collections/Wanfq/fusechat-20-66bf3462c55655c7152e4e23">[HF]</a>
        /
        <a href="https://mp.weixin.qq.com/s/wD9uP33jTZX5jINadcn8IQ">[Êú∫Âô®‰πãÂøÉ]</a>
        /
       <a href="https://github.com/arcee-ai/mergekit/blob/main/mergekit/merge_methods/sce.py">[mergekit]</a>
        <p></p>
        <p>We propose FuseChat, an extended framework of FuseLLM to integrate the collective knowledge and individual strengths of multiple structure- and scale-varied chat LLMs into a more powerful chat LLM. FuseChat-7B is <strong>comparable to the larger Mixtral-8x7B-Instruct and and approaches GPT-3.5-Turbo-1106</strong> on MT-Bench.</p>
      </td>
    </tr>
    </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
    <tr>
      <td colspan="2" style="padding:20px;text-align:left;font-weight:bold;background-color:#ffffff;">
        Large Reasoning Models & Reinforcement Learning
      </td>
    </tr>

      <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/l1.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2505.17667" id="conf_l1">
          <papertitle>QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning</papertitle>
        </a>
        <br>
	Fanqi Wan,
	Weizhou Shen,
	Shengyi Liao,
	Yingcheng Shi,
	Chenliang Li,
	<strong>Ziyi Yang</strong>,
	Ji Zhang,
	Fei Huang,
	Jingren Zhou,
	Ming Yan
        <br>
        <em>Tech Report</em>, 2025
        <br>
        <a href="https://github.com/Tongyi-Zhiwen/QwenLong-L1">[GitHub]</a>
        /
	<a href="https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1-32B">[HF]</a>
        /
        <a href="https://arxiv.org/abs/2505.17667">[Paper]</a>
	/
	<a href="https://www.reddit.com/r/LocalLLaMA/comments/1kvnf46/comment/muau2ne/">[r/LocalLLaMA]</a>
	/
	<a href="https://huggingface.co/papers/2505.17667">[HF Daily Papers]</a>
        <p></p>
        <p>We propose QwenLong-L1, a framework that adapts short-context LRMs to long-context scenarios via progressive context scaling. <strong>QwenLong-L1-32B outperforms flagship LRMs like OpenAI-o3-mini and Qwen3-235B-A22B, achieving performance on par with Claude-3.7-Sonnet-Thinking</strong></p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/thinkswitcher.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://arxiv.org/abs/2505.14183" id="conf_l1">
          <papertitle>ThinkSwitcher: When to Think Hard, When to Think Fast</papertitle>
        </a>
        <br>
	Guosheng Liang,
	Longguang Zhong
	<strong>Ziyi Yang</strong>,
	Xiaojun Quan
        <br>
        <em>Tech Report</em>, 2025
        <br>
        <a href="https://arxiv.org/abs/2505.14183">[Paper]</a>
        <p></p>
        <p>we propose ThinkSwitcher, a framework that enables a single LRM to dynamically switch between short and long CoT modes based on task complexity. ThinkSwitcher <strong>reduces computational cost by 20‚Äì30%</strong> while maintaining high accuracy on complex tasks.</p>
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/fuseo1.png" alt="PontTuset" width="160" style="border-style: none">
      </td>
      <td width="75%" valign="middle">
        <a href="https://huggingface.co/blog/Wanfq/fuseo1-preview" id="conf_fuseo1">
          <papertitle>FuseO1-Preview: System-II Reasoning Fusion of LLMs</papertitle>
        </a>
        <br>
        Fanqi Wan,
	Longguang Zhong,
	<strong>Ziyi Yang</strong>,
	Weizhou Shen,
	Xinting Huang
        <br>
        <em>Tech Report</em>, 2025
        <br>
        <a href="https://github.com/fanqiwan/FuseAI/tree/main/FuseO1-Preview">[GitHub]</a>
        /
	<a href="https://huggingface.co/collections/Wanfq/fuseo1-preview-678eb7c76c337bdf15edef4c">[HF]</a>
        /
        <a href="https://huggingface.co/blog/Wanfq/fuseo1-preview">[Blog]</a>
	/
	<a href="https://www.reddit.com/r/LocalLLaMA/comments/1i81pbk/this_merge_is_amazing/">[r/LocalLLaMA]</a>
	/
	<a href="https://github.com/arcee-ai/mergekit">[Mergekit]</a>
        <p></p>
        <p>FuseO1-Preview is our initial endeavor to enhance the System-II reasoning capabilities of large language models (LLMs) through innovative model fusion techniques. <strong>The resulted FuseO1-DeepSeekR1-QwQ-SkyT1-32B-Preview achieves a Pass@1 accuracy of 74.0 on AIME24, demonstrating significant performance improvements compared to the OpenAI o1-preview (44.6) and OpenAI o1-mini (63.4), even approaching OpenAI o1 (79.2)</strong>.</p>
      </td>
    </tr>

	    
    </tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Internship</heading>
        <p>
          Summer Intern at Tongyi lab, supervised by Dr. Weizhou Shen and Dr. Ming Yan. (2025.5-now).
        </p>
      </td>
    </tr>
    </tbody></table>


    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Education</heading>
        <p>
          MS Student in Computer Technology, <strong>Sun Yat-sen University</strong> (2023.09-2026.06).
        </p>
        <p>
          Bachelor of Computer Science and Technology, <strong>Sun Yat-sen University</strong> (2019.09-2023.06).
        </p>
      </td>
    </tr>
    </tbody></table>


<!--  </tbody></table>-->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">
          Website's code is from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
          
        </p>
      </td>
    </tr>
  </tbody></table>
</td>
</tr>
  </tbody></table>
</body>

</html>
